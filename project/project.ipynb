{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Итоговый проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:38:20.963572Z",
     "start_time": "2020-05-30T12:37:59.170170Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir\n",
    "import codecs\n",
    "import pickle                                                                   \n",
    "from bs4 import BeautifulSoup as BS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:38:20.968850Z",
     "start_time": "2020-05-30T12:38:20.965985Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:38:21.127945Z",
     "start_time": "2020-05-30T12:38:20.972583Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):                                                       \n",
    "    with open(name + '.pkl', 'wb') as f:                                        \n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "                                                                                \n",
    "def load_obj(name ):                                                            \n",
    "    with open(name + '.pkl', 'rb') as f:                                        \n",
    "        return pickle.load(f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:38:22.275489Z",
     "start_time": "2020-05-30T12:38:21.130220Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T08:25:21.541226Z",
     "start_time": "2020-05-30T08:25:21.534117Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatize('русская аба')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:38:45.611084Z",
     "start_time": "2020-05-30T12:38:22.291568Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'NumWordsRulesExtractor'\n",
    "#name = 'ArticleExtractor'\n",
    "name = 'train_data'+name\n",
    "train_data = load_obj(name)#Загружаем готовый распарсенный текст уже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T10:36:57.162246Z",
     "start_time": "2020-05-30T10:14:14.028785Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#это парсинг теста и доп фич (meta)\n",
    "if submit:\n",
    "    test_data= pd.read_csv('test_groups.csv',dtype=np.int16)\n",
    "\n",
    "    parser_type = 'NumWordsRulesExtractor'\n",
    "    def save_obj(obj, name ):                                                       \n",
    "        with open(name + '.pkl', 'wb') as f:                                        \n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "\n",
    "    def load_obj(name ):                                                            \n",
    "        with open(name + '.pkl', 'rb') as f:                                        \n",
    "            return pickle.load(f)                                                   \n",
    "        \n",
    "    description = {}                                                                      \n",
    "    keywords= {}                                                                      \n",
    "    p = {} \n",
    "    h1 = {} \n",
    "    h2 = {} \n",
    "    h3 = {} \n",
    "    h4 = {} \n",
    "    h5 = {} \n",
    "    h6 = {} \n",
    "    test_data['description'] = ''\n",
    "    test_data['keywords'] = ''\n",
    "\n",
    "    path = 'content/'                                                               \n",
    "    texts = {}                                                                      \n",
    "    print('Start: ' + parser_type)                                                  \n",
    "    test_data = pd.read_csv('test_groups.csv',dtype=np.int16)                      \n",
    "    #for filename in ['6770.dat']:                                                  \n",
    "    for filename in tqdm(listdir(path)):                                            \n",
    "        doc_id = int(filename.strip('.dat'))                                        \n",
    "        if doc_id not in test_data.doc_id.values:                                  \n",
    "            continue                                                                \n",
    "        try:                                                                        \n",
    "            with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "                bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "                meta = bs.find(\"meta\", {\"name\":\"description\"})\n",
    "                if meta:\n",
    "                    description[doc_id] = meta.get('content','')\n",
    "                meta = bs.find(\"meta\", {\"name\":\"keywords\"})\n",
    "                if meta:\n",
    "                    keywords[doc_id] = meta.get('content','')\n",
    "                    \n",
    "                p[doc_id] = 0 \n",
    "                h1[doc_id] = 0 \n",
    "                h2[doc_id] = 0 \n",
    "                h3[doc_id] = 0 \n",
    "                h4[doc_id] = 0 \n",
    "                h5[doc_id] = 0 \n",
    "                h6[doc_id] = 0 \n",
    "\n",
    "                p[doc_id] += len(bs.find_all('p'))\n",
    "                h1[doc_id] += len(bs.find_all('h1'))\n",
    "                h2[doc_id] += len(bs.find_all('h2'))\n",
    "                h3[doc_id] += len(bs.find_all('h3'))\n",
    "                h4[doc_id] += len(bs.find_all('h4'))\n",
    "                h5[doc_id] += len(bs.find_all('h5'))\n",
    "                h6[doc_id] += len(bs.find_all('h6'))\n",
    "                \n",
    "                extractor = Extractor(extractor=parser_type, html=html)             \n",
    "                s = extractor.getText()                                             \n",
    "                s=s.replace('\\n',\" \")                                               \n",
    "                s=s.replace('\\t',\" \")                                               \n",
    "                s=s.replace('\\r',\" \")                                               \n",
    "                texts[doc_id] = s                                                   \n",
    "        except UnicodeDecodeError: \n",
    "            print('error {}'.format(filename))                                      \n",
    "            with codecs.open(path + filename, 'r', 'utf-8') as f:                   \n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "            bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "            for s in bs.select('script'):                                           \n",
    "                s.extract()                                                         \n",
    "            s = bs.get_text()                                                       \n",
    "            s=s.replace('\\n',\"\")                                                    \n",
    "            s=s.replace('\\t',\"\")                                                    \n",
    "            s=s.replace('\\r',\"\")                                                    \n",
    "            texts[doc_id] = s                                                       \n",
    "             \n",
    "    test_data['p'] = test_data.apply(lambda row: p.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h1'] = test_data.apply(lambda row: h1.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h2'] = test_data.apply(lambda row: h2.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h3'] = test_data.apply(lambda row: h3.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h4'] = test_data.apply(lambda row: h4.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h5'] = test_data.apply(lambda row: h5.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['h6'] = test_data.apply(lambda row: h6.get(row.doc_id,0), axis = 1)  \n",
    "    test_data['text'] = test_data.apply(lambda row: texts[row.doc_id], axis = 1)  \n",
    "    test_data['description'] = test_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "    test_data['keywords'] = test_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  \n",
    "else:\n",
    "#     path = 'content/'                                                               \n",
    "#     description = {}                                                                      \n",
    "#     keywords= {}                                                                      \n",
    "#     train_data['description'] = ''\n",
    "#     train_data['keywords'] = ''\n",
    "#     #for filename in ['1.dat']:                                                  \n",
    "#     for filename in tqdm(listdir(path)):                                            \n",
    "#         doc_id = int(filename.strip('.dat'))                                        \n",
    "#         if doc_id not in train_data.doc_id.values:                                  \n",
    "#             continue                                                                \n",
    "#         with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "#             url = f.readline().strip()                                          \n",
    "#     #        urls[doc_id] = url                                                       \n",
    "#             html = f.read()                                                     \n",
    "#             bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "#             meta = bs.find(\"meta\", {\"name\":\"description\"})\n",
    "#             if meta:\n",
    "#                 description[doc_id] = lemmatize(meta.get('content',''))\n",
    "#             meta = bs.find(\"meta\", {\"name\":\"keywords\"})\n",
    "#             if meta:\n",
    "#                 keywords[doc_id] = lemmatize(meta.get('content',''))\n",
    "#     train_data['description'] = train_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "#     train_data['keywords'] = train_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  \n",
    "    path = 'content/'                                                               \n",
    "    p = {} \n",
    "    h1 = {} \n",
    "    h2 = {} \n",
    "    h3 = {} \n",
    "    h4 = {} \n",
    "    h5 = {} \n",
    "    h6 = {} \n",
    "    #urls = {}                                                                      \n",
    "#     hrefs_n = {}                                                                      \n",
    "#     hrefs = {}                                                                      \n",
    "#     description = {}                                                                      \n",
    "#     keywords= {}                                                                      \n",
    "#     train_data['description'] = ''\n",
    "#     train_data['keywords'] = ''\n",
    "#     train_data['url'] = ''\n",
    "#     train_data['hrefs'] = ''\n",
    "#     train_data['hrefs_n'] = 0\n",
    "#    for filename in ['15731.dat']:                                                  \n",
    "    for filename in tqdm(listdir(path)):                                            \n",
    "        doc_id = int(filename.strip('.dat'))                                        \n",
    "        if doc_id not in train_data.doc_id.values:                                  \n",
    "            continue                                                                \n",
    "        with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            url = f.readline().strip()                                          \n",
    "            #urls[doc_id] = re.split(';|,|\\.|\\_|\\-|\\*|\\/|\\&|\\?|\\=',url)\n",
    "            html = f.read()                                                     \n",
    "            bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "#             meta = bs.find(\"meta\", {\"name\":\"description\"})\n",
    "#             if meta:\n",
    "#                 description[doc_id] = lemmatize(meta.get('content',''))\n",
    "#             meta = bs.find(\"meta\", {\"name\":\"keywords\"})\n",
    "#             if meta:\n",
    "#                 keywords[doc_id] = lemmatize(meta.get('content',''))\n",
    "#             hrefs_n[doc_id] = 0 \n",
    "#             hrefs[doc_id] = []                                                       \n",
    "           \n",
    "#             hrefs_bs = bs.find_all(href=True)\n",
    "#             for href in hrefs_bs:\n",
    "#                 if href:\n",
    "#                     if href['href'][0:4] == 'http':\n",
    "#                         hrefs[doc_id].append(re.split(';|,|\\.|\\_|\\-|\\*|\\/|\\&|\\?|\\=',href['href']))\n",
    "#                         hrefs_n[doc_id] +=1\n",
    "            p[doc_id] = 0 \n",
    "            h1[doc_id] = 0 \n",
    "            h2[doc_id] = 0 \n",
    "            h3[doc_id] = 0 \n",
    "            h4[doc_id] = 0 \n",
    "            h5[doc_id] = 0 \n",
    "            h6[doc_id] = 0 \n",
    "            \n",
    "            p[doc_id] += len(bs.find_all('p'))\n",
    "            h1[doc_id] += len(bs.find_all('h1'))\n",
    "            h2[doc_id] += len(bs.find_all('h2'))\n",
    "            h3[doc_id] += len(bs.find_all('h3'))\n",
    "            h4[doc_id] += len(bs.find_all('h4'))\n",
    "            h5[doc_id] += len(bs.find_all('h5'))\n",
    "            h6[doc_id] += len(bs.find_all('h6'))\n",
    "             \n",
    "    train_data['p'] = train_data.apply(lambda row: p.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h1'] = train_data.apply(lambda row: h1.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h2'] = train_data.apply(lambda row: h2.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h3'] = train_data.apply(lambda row: h3.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h4'] = train_data.apply(lambda row: h4.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h5'] = train_data.apply(lambda row: h5.get(row.doc_id,0), axis = 1)  \n",
    "    train_data['h6'] = train_data.apply(lambda row: h6.get(row.doc_id,0), axis = 1)  \n",
    "                        \n",
    "                \n",
    "#     train_data['url'] = train_data.apply(lambda row: urls.get(row.doc_id,''), axis = 1)  \n",
    "#     train_data['hrefs'] = train_data.apply(lambda row: hrefs.get(row.doc_id,''), axis = 1)  \n",
    "#     train_data['hrefs_n'] = train_data.apply(lambda row: hrefs_n.get(row.doc_id,''), axis = 1)  \n",
    "#     train_data['description'] = train_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "#     train_data['keywords'] = train_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T08:49:36.046669Z",
     "start_time": "2020-05-30T08:49:36.043115Z"
    }
   },
   "outputs": [],
   "source": [
    "len(description),len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T08:49:40.948945Z",
     "start_time": "2020-05-30T08:49:36.048600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "save_obj(urls, 'urls')\n",
    "save_obj(hrefs, 'hrefs')\n",
    "save_obj(hrefs_n, 'hrefs_n')\n",
    "save_obj(description, 'description')\n",
    "save_obj(keywords, 'keywords')\n",
    "save_obj(p, 'p')\n",
    "save_obj(h1, 'h1')\n",
    "save_obj(h2, 'h2')\n",
    "save_obj(h3, 'h3')\n",
    "save_obj(h4, 'h4')\n",
    "save_obj(h5, 'h5')\n",
    "save_obj(h6, 'h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:33.852372Z",
     "start_time": "2020-05-30T12:41:29.619116Z"
    }
   },
   "outputs": [],
   "source": [
    "urls=load_obj( 'urls')\n",
    "hrefs=load_obj( 'hrefs')\n",
    "hrefs_n=load_obj( 'hrefs_n')\n",
    "description= load_obj( 'description')\n",
    "keywords = load_obj( 'keywords')\n",
    "p =load_obj( 'p')\n",
    "h1 =load_obj( 'h1')\n",
    "h2 =load_obj( 'h2')\n",
    "h3 =load_obj( 'h3')\n",
    "h4 =load_obj( 'h4')\n",
    "h5 =load_obj( 'h5')\n",
    "h6 =load_obj( 'h6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:37.019427Z",
     "start_time": "2020-05-30T12:41:33.855079Z"
    }
   },
   "outputs": [],
   "source": [
    "#если загружать\n",
    "train_data['p'] = train_data.apply(lambda row: p.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h1'] = train_data.apply(lambda row: h1.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h2'] = train_data.apply(lambda row: h2.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h3'] = train_data.apply(lambda row: h3.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h4'] = train_data.apply(lambda row: h4.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h5'] = train_data.apply(lambda row: h5.get(row.doc_id,0), axis = 1)  \n",
    "train_data['h6'] = train_data.apply(lambda row: h6.get(row.doc_id,0), axis = 1)  \n",
    "                        \n",
    "                \n",
    "train_data['url'] = train_data.apply(lambda row: urls.get(row.doc_id,''), axis = 1)  \n",
    "train_data['hrefs'] = train_data.apply(lambda row: hrefs.get(row.doc_id,''), axis = 1)  \n",
    "train_data['hrefs_n'] = train_data.apply(lambda row: hrefs_n.get(row.doc_id,''), axis = 1)  \n",
    "train_data['description'] = train_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "train_data['keywords'] = train_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:38:14.413980Z",
     "start_time": "2020-05-30T18:38:13.259623Z"
    }
   },
   "outputs": [],
   "source": [
    "new_meta = load_obj('tem1')\n",
    "new_meta2 = load_obj('themes_Dasha')\n",
    "new_meta3 = load_obj('download2_Anya_')\n",
    "keys = list(new_meta3.keys())\n",
    "for key in keys:\n",
    "    new_meta3[int(key)]=new_meta3[key]\n",
    "    del new_meta3[key]\n",
    "new_meta.update(new_meta2)\n",
    "new_meta.update(new_meta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:38:14.851000Z",
     "start_time": "2020-05-30T18:38:14.416411Z"
    }
   },
   "outputs": [],
   "source": [
    "for key in new_meta.keys():\n",
    "    s = new_meta[key]\n",
    "    s = ' '.join(s)\n",
    "    s = s[:s.find('Совпад')]\n",
    "    s=s.replace('\\n',\" \")                                                    \n",
    "    s=s.replace('\\t',\" \")  \n",
    "    s=s.replace('.',\" \")  \n",
    "    s=s.replace('%',\" \")  \n",
    "    \n",
    "    s = ''.join(i for i in s if not i.isdigit())\n",
    "    s=s.replace('Тематика:',\" \")  \n",
    "    s=s.replace('Совпадение:',\" \")  \n",
    "    s = s.strip()\n",
    "    s=s.replace(' ',\"а\")  \n",
    "    s=s.replace('/',\" \")  \n",
    "    new_meta[key] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T20:25:51.796600Z",
     "start_time": "2020-05-30T20:25:51.792719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n                        Тематика:\\n                                                    Top/Computers/Programming\\n\\n\\n                        Совпадение:\\n                        60.1%\\n\\t\\t\\t\\t\\t\\n',\n",
       " '\\n\\n                        Тематика:\\n                                                    Top/Business/News_and_Media\\n\\n\\n                        Совпадение:\\n                        54.3%\\n\\t\\t\\t\\t\\t\\n',\n",
       " '\\n\\n                        Тематика:\\n                                                    Top/Computers/Hacking\\n\\n\\n                        Совпадение:\\n                        53.7%\\n\\t\\t\\t\\t\\t\\n']"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_meta3[10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:38:19.218335Z",
     "start_time": "2020-05-30T18:38:17.566930Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['new_meta'] = train_data.apply(lambda row: new_meta.get(row.doc_id,''), axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T07:49:31.252776Z",
     "start_time": "2020-05-30T07:49:10.236401Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_obj(train_data, name + 'lem_meta_hrefs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:37.119447Z",
     "start_time": "2020-05-30T12:41:37.021427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11690\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding = 'UTF-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "if submit:\n",
    "    test_doc_titles = []\n",
    "    for id in test_data.doc_id.values:\n",
    "        test_doc_titles.append(doc_to_title[id])\n",
    "    test_data['title'] = test_doc_titles\n",
    "    print(len(test_doc_titles))\n",
    "else:\n",
    "    doc_titles = []\n",
    "    for id in train_data.doc_id.values:\n",
    "        doc_titles.append(doc_to_title[id])\n",
    "    train_data['title'] = doc_titles\n",
    "    print(len(doc_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:56.930048Z",
     "start_time": "2020-05-30T12:41:38.955445Z"
    }
   },
   "outputs": [],
   "source": [
    "if submit:\n",
    "    test_data['lem_title'] = test_data.apply(lambda row:lemmatize(row.title) , axis = 1)  \n",
    "else:\n",
    "    train_data['lem_title'] = train_data.apply(lambda row:lemmatize(row.title) , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:57.419523Z",
     "start_time": "2020-05-30T12:41:56.933421Z"
    }
   },
   "outputs": [],
   "source": [
    "if submit:\n",
    "    test_data['sum_title'] = test_data.apply(lambda row: row.lem_title +row.description + row.keywords , axis = 1)  \n",
    "else:\n",
    "    train_data['sum_title'] = train_data.apply(lambda row: row.lem_title +row.description + row.keywords , axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:43:08.545294Z",
     "start_time": "2020-05-28T12:43:08.543080Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "# train_data['text'] = train_data.progress_apply(lambda row: re.sub(r'\\d+', '', row.text) , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:57.425745Z",
     "start_time": "2020-05-30T12:41:57.421437Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_splits(data, n=3, random_state=None):\n",
    "    groups = train_data.group_id.unique()\n",
    "    kf = KFold(n_splits=n, shuffle=True, random_state=random_state)\n",
    "    for i,j in kf.split(groups):\n",
    "        train, test = data.group_id.isin(groups[i].reshape(-1)), data.group_id.isin(groups[j].reshape(-1))\n",
    "        yield (train[train].index.values, test[test].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T12:41:57.515460Z",
     "start_time": "2020-05-30T12:41:57.427681Z"
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(ngram_range = (1,3))#max_features = 70)\n",
    "stop_words =['без','в','во','для','до','за','И','из','к','ко','меж','на','над','о','об','обо','от','ото','по','под','при','про','с', 'а','же','и','ибо','или','как','но','раз','чем','что','не','ни','ли','ль','вот','вон','то','ну','как','же','ни','уж','ну','я', 'ты', 'он', 'мы', 'вы', 'её', 'их','кто','тот']\n",
    "\n",
    " \n",
    "#vectorizer = TfidfVectorizer(preprocessor=lemmatize,stop_words=stop_words)#max_features = 70)\n",
    "vectorizer = CountVectorizer()#max_features = 70)\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_words)#max_features = 70)\n",
    "#vectorizer = TfidfVectorizer()#max_features = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T14:17:18.252991Z",
     "start_time": "2020-05-30T14:17:18.241099Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1],\n",
       "       [  1],\n",
       "       [  1],\n",
       "       ...,\n",
       "       [129],\n",
       "       [129],\n",
       "       [129]], dtype=int16)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_groups = np.array(list(range(1,310))).reshape(-1,1)\n",
    "i = OneHotEncoder()\n",
    "i.fit(all_groups)\n",
    "if submit:\n",
    "    groups_one_hot_test   = i.transform(test_data.group_id.values.reshape(-1,1))\n",
    "else:\n",
    "    groups_one_hot_train = i.transform(train_data.group_id.values.reshape(-1,1))\n",
    "train_data.group_id.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T15:34:11.622842Z",
     "start_time": "2020-05-30T15:34:11.617858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top Computers Programming                                                                                                                                               Top Games Video_Games                                                                                                                                               Top Computers Emulators                                                              '"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.new_meta.iloc[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T07:54:48.873111Z",
     "start_time": "2020-05-30T07:54:48.864081Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "\n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(stop_words=stop_words).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [i[0] for i in words_freq[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T09:44:46.889675Z",
     "start_time": "2020-05-30T09:44:46.110268Z"
    }
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist[1:3]]\n",
    "train_data['hrefs'] = train_data.apply(lambda row: flatten(row.hrefs), axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T09:31:08.530956Z",
     "start_time": "2020-05-30T09:31:08.219728Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['url'] = train_data.apply(lambda row: ' '.join(row.url), axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:53:08.491186Z",
     "start_time": "2020-05-30T18:53:08.438498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "vectorizer2.fit(train_data.new_meta)\n",
    "#vectorizer2 = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T20:28:02.907087Z",
     "start_time": "2020-05-30T20:28:02.259930Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    number = 1\n",
    "    data['first_pop'] =0\n",
    "    for group in tqdm(data.group_id.unique()):\n",
    "        number +=1\n",
    "        elements_in_same_group = data.group_id == group\n",
    "        #words = data.title[elements_in_same_group] + data.text[elements_in_same_group]\n",
    "        words = data.lem_title[elements_in_same_group] #data.lem_title[elements_in_same_group]\n",
    "        vectorizer.fit(words)\n",
    "        texts_vectorized_titles = vectorizer.transform(data.lem_title[elements_in_same_group])\n",
    "        texts_vectorized_titles = scaler.fit_transform(texts_vectorized_titles)\n",
    "        texts_vectorized_texts = vectorizer.transform(data.text[elements_in_same_group])\n",
    "        texts_vectorized_texts = scaler.fit_transform(texts_vectorized_texts)\n",
    "        texts_vectorized_keywords = vectorizer.transform(data.keywords[elements_in_same_group])\n",
    "        texts_vectorized_keywords = scaler.fit_transform(texts_vectorized_keywords)\n",
    "        texts_vectorized_description= vectorizer.transform(data.description[elements_in_same_group])\n",
    "        texts_vectorized_description= scaler.fit_transform(texts_vectorized_description)\n",
    "    \n",
    "        texts_vectorized_sumtitles = vectorizer.transform(data.sum_title[elements_in_same_group])\n",
    "        texts_vectorized_sumtitles = scaler.fit_transform(texts_vectorized_sumtitles)\n",
    "        \n",
    "# #        vectorizer2.fit(data.new_meta[elements_in_same_group])\n",
    "#        texts_vectorized_new_meta= vectorizer2.transform(data.new_meta[elements_in_same_group])\n",
    "#        texts_vectorized_new_meta = csr_matrix(np.abs(texts_vectorized_new_meta.toarray()-1))\n",
    "#        texts_vectorized_new_meta= scaler.fit_transform(texts_vectorized_new_meta)\n",
    "#         urls = data.url[elements_in_same_group] + data.hrefs[elements_in_same_group]\n",
    "#         urls = data.hrefs[elements_in_same_group]\n",
    "# #        print(urls)\n",
    "#         vectorizer2 = TfidfVectorizer()#max_features = 70)\n",
    "#         vectorizer2.fit(urls)\n",
    "#         texts_vectorized_urls = vectorizer2.transform(urls)\n",
    "#         texts_vectorized_urls = scaler.fit_transform(texts_vectorized_urls)\n",
    "        \n",
    "        dist_matr = pairwise_distances(texts_vectorized_titles, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr2 = pairwise_distances(texts_vectorized_texts, texts_vectorized_texts, metric = 'cosine')\n",
    "        dist_matr3 = pairwise_distances(texts_vectorized_texts, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr4 = pairwise_distances(texts_vectorized_keywords, texts_vectorized_keywords, metric = 'cosine')\n",
    "        dist_matr5 = pairwise_distances(texts_vectorized_description, texts_vectorized_description, metric = 'cosine')\n",
    "        dist_matr6 = pairwise_distances(texts_vectorized_sumtitles, texts_vectorized_texts, metric = 'cosine')\n",
    "        dist_matr7 = pairwise_distances(texts_vectorized_keywords, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr8 = pairwise_distances(texts_vectorized_description, texts_vectorized_titles, metric = 'cosine')\n",
    "        #clustering = DBSCAN(eps = 1.3, min_samples=5).fit(dist_matr)\n",
    "        #clf = IsolationForest(random_state=0).fit_predict(dist_matr)\n",
    "\n",
    "        dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:21]\n",
    "        dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:21]\n",
    "        dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:21]\n",
    "        dist_matr_sorted4 = np.sort(dist_matr4,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted5 = np.sort(dist_matr5,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted6 = np.sort(dist_matr6,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted7 = np.sort(dist_matr7,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted8 = np.sort(dist_matr8,axis=1)[:,:-1][:,0:21]\n",
    "#         dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:min(50,dist_matr.shape[1])]\n",
    "#         dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:min(50,dist_matr2.shape[1])]\n",
    "#         dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:min(50,dist_matr3.shape[1])]\n",
    "\n",
    "#         if dist_matr_sorted.shape[1] < 50:\n",
    "#             dist_matr_sorted = np.hstack((dist_matr_sorted, np.tile(dist_matr_sorted[:,-1:], (1, 50-dist_matr_sorted.shape[1]))))\n",
    "#             dist_matr_sorted2 = np.hstack((dist_matr_sorted2, np.tile(dist_matr_sorted2[:,-1:], (1, 50-dist_matr_sorted2.shape[1]))))\n",
    "#             dist_matr_sorted3 = np.hstack((dist_matr_sorted3, np.tile(dist_matr_sorted3[:,-1:], (1, 50-dist_matr_sorted3.shape[1]))))\n",
    "\n",
    "\n",
    "        mean_dist = dist_matr.mean(axis = 1)\n",
    "        mean_dist2 = dist_matr2.mean(axis = 1)\n",
    "        mean_dist3 = dist_matr3.mean(axis = 1)\n",
    "        mean_dist4 = dist_matr4.mean(axis = 1)\n",
    "        mean_dist5 = dist_matr5.mean(axis = 1)\n",
    "        mean_dist6 = dist_matr6.mean(axis = 1)\n",
    "        mean_dist7 = dist_matr7.mean(axis = 1)\n",
    "        mean_dist8 = dist_matr8.mean(axis = 1)\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist2.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist3.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist4.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist5.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist6.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist7.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist8.reshape(-1,1)))\n",
    "        \n",
    "#         pop_words = get_top_n_words(words,n=100)\n",
    "#         for pop_word in pop_words:\n",
    "#             data.loc[elements_in_same_group,'first_pop']= data[elements_in_same_group].apply(lambda row: int(pop_word in row.text) , axis = 1)  \n",
    "#             pop  = csr_matrix(data[elements_in_same_group].first_pop.values)\n",
    "#             print(pop_word,pop.toarray())\n",
    "#             dist_matr_sorted = np.hstack((dist_matr_sorted, pop.toarray().reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted2))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted3))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted4))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted5))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted6))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted7))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted8))\n",
    "        n =  data.hrefs_n[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.p[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h1[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h2[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h3[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h4[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h5[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "        n =  data.h6[elements_in_same_group].values\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, n.reshape(-1,1)))\n",
    "# #        dist_matr_sorted = np.hstack((dist_matr_sorted, clustering.labels_.reshape(-1,1)))\n",
    "      #  dist_matr_sorted = np.hstack((dist_matr_sorted, texts_vectorized_new_meta.toarray()))\n",
    "\n",
    "        if number == 2 :\n",
    "            features_by_dist =  dist_matr_sorted\n",
    "        else:\n",
    "            features_by_dist = vstack((features_by_dist, dist_matr_sorted))\n",
    "    return features_by_dist.tocsr()\n",
    "            \n",
    "#         print(vectorizer.get_feature_names())\n",
    "#         print(word_vectors_s2['ваза'])\n",
    "#         coord = np.array([word_vectors_s2[i] for i in word_vectors_s2.index2entity[start_wn:start_wn+words_n]])\n",
    "#         fig, ax = plt.subplots()\n",
    "#         coord = coord.T\n",
    "#         ax.scatter(coord[0],coord[1])\n",
    "#         for i in range(words_n):\n",
    "#             plt.annotate(word_vectors_s2.index2entity[start_wn+i], (coord[0][i],coord[1][i]) )\n",
    "#         plt.show()\n",
    "#         break\n",
    "#         vector_size=5\n",
    "#         e = []\n",
    "#         e.append(list(vectorizer.vocabulary_.keys()))\n",
    "#         w2v = Word2Vec(e, min_count=1, size=vector_size, iter=10)\n",
    "#         word_vectors_s2 = w2v.wv\n",
    "#         #print(data.lem_title[elements_in_same_group][0])\n",
    "#         vectors = []\n",
    "#         for i in vectorizer.inverse_transform(texts_vectorized_texts):\n",
    "#             tmp2 = np.zeros(vector_size,dtype=np.float64)\n",
    "#             for j in i[:3]:\n",
    "#                 tmp2 += np.array(word_vectors_s2[j])\n",
    "#                 break\n",
    "# #             if(len(i)!=0):\n",
    "# #                 tmp2/len(i)\n",
    "#             vectors.append(tmp2)\n",
    "                \n",
    "#         new = np.array(vectors)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T20:29:02.225291Z",
     "start_time": "2020-05-30T20:28:02.909646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffd18b063f8464ca5025a61085a2d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if submit:\n",
    "    features_by_dist = get_features(test_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_test])\n",
    "    features_by_dist = features_by_dist.tocsr()\n",
    "else:\n",
    "    features_by_dist = get_features(train_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_train])\n",
    "    features_by_dist = features_by_dist.tocsr()\n",
    "    target = train_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-30T20:29:05.413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d768c1e78b343ddaf78e17579bba89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7542955326460482 0.7695150115473441 0.7397397397397396 \n",
      "0.7294569918308506 0.7547495682210708 0.7420289855072464 \n",
      "0.7911290322580646 0.7591318157755426 0.6893939393939393 \n",
      "0.7940074906367041 0.7447698744769875 0.7174472465259907 \n",
      "0.7183348095659876 0.7554060547813551 0.782051282051282 \n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#VALIDATION\n",
    "max_score = 0\n",
    "scores = []\n",
    "pca = PCA(n_components=4)\n",
    "#j = pca.fit_transform(features_by_dist.toarray())\n",
    "j = features_by_dist\n",
    "thre = False \n",
    "for i in tqdm(range(20)):\n",
    "    \n",
    "    for train, test in get_splits(train_data,n=3,random_state=i):\n",
    "        if thre:\n",
    "            pass\n",
    "    #        model1 = RandomForestClassifier(n_jobs=6,n_estimators=80)\n",
    "            #model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.2, max_depth=9)\n",
    "    #        model2 = XGBClassifier()\n",
    "    #        model3 = LogisticRegression(n_jobs=6)\n",
    "    #        model.fit(features_by_dist[train], target[train])\n",
    "    #        model1.fit(j[train], target[train])\n",
    "    #        model2.fit(j[train], target[train])\n",
    "    #        model3.fit(j[train], target[train])\n",
    "#             pred1 = model.predict(j[test])               \n",
    "#             pred2 = model.predict(j[test])               \n",
    "#             pred3 = model.predict(j[test])               \n",
    "#             pred = (pred1 + pred2+pred3)>1\n",
    "#             pred = pred.astype(int)\n",
    "#            pred = model.predict(j[test])               \n",
    "        else:\n",
    "            #model = KNeighborsClassifier(n_neighbors=3)\n",
    "            model = RandomForestClassifier(n_jobs=6,n_estimators=1000)\n",
    "            model.fit(j[train], target[train])\n",
    "            pred = model.predict(j[test])               \n",
    "        score = f1_score(pred,train_data.target[test])\n",
    "        scores.append(score)\n",
    "        print(score, end=' ')\n",
    "    print()\n",
    "\n",
    "print('\\nAvg score:' ,sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T05:07:35.189175Z",
     "start_time": "2020-05-30T05:07:26.930552Z"
    }
   },
   "outputs": [],
   "source": [
    "#SAVE model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# model = RandomForestClassifier(n_jobs=6,n_estimators=1000)\n",
    "# model.fit(features_by_dist, target)\n",
    "# save_obj(model, 'model')\n",
    "\n",
    "model1 = RandomForestClassifier(n_jobs=6,n_estimators=80)\n",
    "#model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.2, max_depth=9)\n",
    "model2 = XGBClassifier()\n",
    "model3 = LogisticRegression(n_jobs=6)\n",
    "#        model.fit(features_by_dist[train], target[train])\n",
    "model1.fit(features_by_dist, target)\n",
    "model2.fit(features_by_dist, target)\n",
    "model3.fit(features_by_dist, target)\n",
    "save_obj(model1, 'model1')\n",
    "save_obj(model2, 'model2')\n",
    "save_obj(model3, 'model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T05:54:48.528834Z",
     "start_time": "2020-05-30T05:54:46.034824Z"
    }
   },
   "outputs": [],
   "source": [
    "#SUBMIT\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# print(type(features_by_dist))\n",
    "# model = load_obj('model') \n",
    "# pred = model.predict(features_by_dist)               \n",
    "# pred.shape\n",
    "model1 = load_obj('model1') \n",
    "model2 = load_obj('model2') \n",
    "model3 = load_obj('model3') \n",
    "pred1 = model1.predict(features_by_dist)               \n",
    "pred2 = model2.predict(features_by_dist)               \n",
    "pred3 = model3.predict(features_by_dist)               \n",
    "pred = (pred1 + pred2+pred3)>1\n",
    "pred = pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T05:57:19.100850Z",
     "start_time": "2020-05-30T05:57:18.839914Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['target'] = pred\n",
    "test_data.to_csv('sub_3with_meta',index=False,columns=['pair_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T20:08:37.104082Z",
     "start_time": "2020-05-27T20:08:35.524395Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "gr = int(input())\n",
    "print(gr)\n",
    "pca = PCA(n_components=components=2)\n",
    "i = pca.fit_transform(features_by_dist[train_data[train_data.group_id==gr].index.values].toarray())\n",
    "a = plt.subplot()\n",
    "a.scatter(i[:, 0], i[:, 1], c=target[train_data[train_data.group_id==gr].index.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T19:33:16.344Z"
    }
   },
   "outputs": [],
   "source": [
    "save_obj(test_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
