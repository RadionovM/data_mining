{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Итоговый проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:02:40.666204Z",
     "start_time": "2020-05-24T13:02:18.463933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jpype/_core.py:209: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir\n",
    "import codecs\n",
    "import pickle                                                                   \n",
    "from bs4 import BeautifulSoup as BS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from boilerpipe.extract import Extractor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:02:40.869635Z",
     "start_time": "2020-05-24T13:02:40.670052Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv',dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:27:27.866593Z",
     "start_time": "2020-05-24T13:27:27.845753Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test_data= pd.read_csv('test_groups.csv',dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:48:34.201028Z",
     "start_time": "2020-05-24T13:27:49.267712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: NumWordsRulesExtractor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 13063/28026 [10:07<17:59, 13.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 6770.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28026/28026 [20:44<00:00, 22.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#это парсинг теста\n",
    "import pandas as pd                                                             \n",
    "import numpy as np                                                              \n",
    "from tqdm import tqdm                                                           \n",
    "from os import listdir                                                          \n",
    "import codecs                                                                   \n",
    "import pickle                                                                   \n",
    "from bs4 import BeautifulSoup as BS                                             \n",
    "from boilerpipe.extract import Extractor                                        \n",
    "from sys import argv                                                            \n",
    "                                                                                \n",
    "parser_type = 'NumWordsRulesExtractor'\n",
    "def save_obj(obj, name ):                                                       \n",
    "    with open(name + '.pkl', 'wb') as f:                                        \n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "                                                                                \n",
    "def load_obj(name ):                                                            \n",
    "    with open(name + '.pkl', 'rb') as f:                                        \n",
    "        return pickle.load(f)                                                   \n",
    "                                                                                \n",
    "path = 'content/'                                                               \n",
    "texts = {}                                                                      \n",
    "print('Start: ' + parser_type)                                                  \n",
    "test_data = pd.read_csv('test_groups.csv',dtype=np.int16)                      \n",
    "#for filename in ['6770.dat']:                                                  \n",
    "for filename in tqdm(listdir(path)):                                            \n",
    "    doc_id = int(filename.strip('.dat'))                                        \n",
    "    if doc_id not in test_data.doc_id.values:                                  \n",
    "        continue                                                                \n",
    "    try:                                                                        \n",
    "        with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            url = f.readline().strip()                                          \n",
    "            html = f.read()                                                     \n",
    "            extractor = Extractor(extractor=parser_type, html=html)             \n",
    "            s = extractor.getText()                                             \n",
    "            s=s.replace('\\n',\" \")                                               \n",
    "            s=s.replace('\\t',\" \")                                               \n",
    "            s=s.replace('\\r',\" \")                                               \n",
    "            texts[doc_id] = s                                                   \n",
    "    except UnicodeDecodeError: \n",
    "        print('error {}'.format(filename))                                      \n",
    "        with codecs.open(path + filename, 'r', 'utf-8') as f:                   \n",
    "            url = f.readline().strip()                                          \n",
    "            html = f.read()                                                     \n",
    "        bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "        for s in bs.select('script'):                                           \n",
    "            s.extract()                                                         \n",
    "        s = bs.get_text()                                                       \n",
    "        s=s.replace('\\n',\"\")                                                    \n",
    "        s=s.replace('\\t',\"\")                                                    \n",
    "        s=s.replace('\\r',\"\")                                                    \n",
    "        texts[doc_id] = s                                                       \n",
    "test_data['text'] = test_data.apply(lambda row: texts[row.doc_id], axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:02:40.985488Z",
     "start_time": "2020-05-24T13:02:40.879097Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):                                                       \n",
    "    with open(name + '.pkl', 'wb') as f:                                        \n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "                                                                                \n",
    "def load_obj(name ):                                                            \n",
    "    with open(name + '.pkl', 'rb') as f:                                        \n",
    "        return pickle.load(f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:02:41.085019Z",
     "start_time": "2020-05-24T13:02:40.993380Z"
    }
   },
   "outputs": [],
   "source": [
    "#START HEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:02:41.163122Z",
     "start_time": "2020-05-24T13:02:41.088553Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'NumWordsRulesExtractor'\n",
    "name = 'train_data'+name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:03:04.852015Z",
     "start_time": "2020-05-24T13:02:41.165606Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = load_obj(name)#Загружаем готовый распарсенный текст уже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:03:27.478974Z",
     "start_time": "2020-05-24T13:03:27.455656Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:18.603829Z",
     "start_time": "2020-05-24T13:49:18.284046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding = 'UTF-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:28.015210Z",
     "start_time": "2020-05-24T13:49:27.986110Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_titles = []\n",
    "for id in train_data.doc_id.values:\n",
    "    doc_titles.append(doc_to_title[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:28.604052Z",
     "start_time": "2020-05-24T13:49:28.310433Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['title'] = doc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:20.380048Z",
     "start_time": "2020-05-24T13:49:20.334685Z"
    }
   },
   "outputs": [],
   "source": [
    "test_doc_titles = []\n",
    "for id in test_data.doc_id.values:\n",
    "    test_doc_titles.append(doc_to_title[id])\n",
    "test_data['title'] = test_doc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:30.843134Z",
     "start_time": "2020-05-24T13:49:30.837677Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_splits(data,n=3):\n",
    "    bound =[]\n",
    "    max_group = data.group_id.max()\n",
    "    bound.extend([i*int(max_group/n)+1 for i in range(1,n)])\n",
    "    bound.append(max_group+1)\n",
    "    prev_id = 1\n",
    "    for group_id in bound:\n",
    "        yield (data[~((prev_id <= data.group_id) & (data.group_id < group_id))].pair_id.values-1,\n",
    "            data[(prev_id <= data.group_id) & (data.group_id < group_id)].pair_id.values-1)\n",
    "        prev_id = group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:56:46.512357Z",
     "start_time": "2020-05-24T13:56:46.494087Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()#max_features = 70)\n",
    "#texts_vectorized = vectorizer.fit_transform(train_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:56:46.955516Z",
     "start_time": "2020-05-24T13:56:46.950475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130, 130, 130, ..., 309, 309, 309], dtype=int16)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.group_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:35.530456Z",
     "start_time": "2020-05-24T13:49:35.527118Z"
    }
   },
   "outputs": [],
   "source": [
    "groups_tmp = np.concatenate((train_data.group_id.values,  test_data.group_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:57:43.266515Z",
     "start_time": "2020-05-24T13:57:43.259379Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1],\n",
       "       [  2],\n",
       "       [  3],\n",
       "       [  4],\n",
       "       [  5],\n",
       "       [  6],\n",
       "       [  7],\n",
       "       [  8],\n",
       "       [  9],\n",
       "       [ 10],\n",
       "       [ 11],\n",
       "       [ 12],\n",
       "       [ 13],\n",
       "       [ 14],\n",
       "       [ 15],\n",
       "       [ 16],\n",
       "       [ 17],\n",
       "       [ 18],\n",
       "       [ 19],\n",
       "       [ 20],\n",
       "       [ 21],\n",
       "       [ 22],\n",
       "       [ 23],\n",
       "       [ 24],\n",
       "       [ 25],\n",
       "       [ 26],\n",
       "       [ 27],\n",
       "       [ 28],\n",
       "       [ 29],\n",
       "       [ 30],\n",
       "       [ 31],\n",
       "       [ 32],\n",
       "       [ 33],\n",
       "       [ 34],\n",
       "       [ 35],\n",
       "       [ 36],\n",
       "       [ 37],\n",
       "       [ 38],\n",
       "       [ 39],\n",
       "       [ 40],\n",
       "       [ 41],\n",
       "       [ 42],\n",
       "       [ 43],\n",
       "       [ 44],\n",
       "       [ 45],\n",
       "       [ 46],\n",
       "       [ 47],\n",
       "       [ 48],\n",
       "       [ 49],\n",
       "       [ 50],\n",
       "       [ 51],\n",
       "       [ 52],\n",
       "       [ 53],\n",
       "       [ 54],\n",
       "       [ 55],\n",
       "       [ 56],\n",
       "       [ 57],\n",
       "       [ 58],\n",
       "       [ 59],\n",
       "       [ 60],\n",
       "       [ 61],\n",
       "       [ 62],\n",
       "       [ 63],\n",
       "       [ 64],\n",
       "       [ 65],\n",
       "       [ 66],\n",
       "       [ 67],\n",
       "       [ 68],\n",
       "       [ 69],\n",
       "       [ 70],\n",
       "       [ 71],\n",
       "       [ 72],\n",
       "       [ 73],\n",
       "       [ 74],\n",
       "       [ 75],\n",
       "       [ 76],\n",
       "       [ 77],\n",
       "       [ 78],\n",
       "       [ 79],\n",
       "       [ 80],\n",
       "       [ 81],\n",
       "       [ 82],\n",
       "       [ 83],\n",
       "       [ 84],\n",
       "       [ 85],\n",
       "       [ 86],\n",
       "       [ 87],\n",
       "       [ 88],\n",
       "       [ 89],\n",
       "       [ 90],\n",
       "       [ 91],\n",
       "       [ 92],\n",
       "       [ 93],\n",
       "       [ 94],\n",
       "       [ 95],\n",
       "       [ 96],\n",
       "       [ 97],\n",
       "       [ 98],\n",
       "       [ 99],\n",
       "       [100],\n",
       "       [101],\n",
       "       [102],\n",
       "       [103],\n",
       "       [104],\n",
       "       [105],\n",
       "       [106],\n",
       "       [107],\n",
       "       [108],\n",
       "       [109],\n",
       "       [110],\n",
       "       [111],\n",
       "       [112],\n",
       "       [113],\n",
       "       [114],\n",
       "       [115],\n",
       "       [116],\n",
       "       [117],\n",
       "       [118],\n",
       "       [119],\n",
       "       [120],\n",
       "       [121],\n",
       "       [122],\n",
       "       [123],\n",
       "       [124],\n",
       "       [125],\n",
       "       [126],\n",
       "       [127],\n",
       "       [128],\n",
       "       [129],\n",
       "       [130],\n",
       "       [131],\n",
       "       [132],\n",
       "       [133],\n",
       "       [134],\n",
       "       [135],\n",
       "       [136],\n",
       "       [137],\n",
       "       [138],\n",
       "       [139],\n",
       "       [140],\n",
       "       [141],\n",
       "       [142],\n",
       "       [143],\n",
       "       [144],\n",
       "       [145],\n",
       "       [146],\n",
       "       [147],\n",
       "       [148],\n",
       "       [149],\n",
       "       [150],\n",
       "       [151],\n",
       "       [152],\n",
       "       [153],\n",
       "       [154],\n",
       "       [155],\n",
       "       [156],\n",
       "       [157],\n",
       "       [158],\n",
       "       [159],\n",
       "       [160],\n",
       "       [161],\n",
       "       [162],\n",
       "       [163],\n",
       "       [164],\n",
       "       [165],\n",
       "       [166],\n",
       "       [167],\n",
       "       [168],\n",
       "       [169],\n",
       "       [170],\n",
       "       [171],\n",
       "       [172],\n",
       "       [173],\n",
       "       [174],\n",
       "       [175],\n",
       "       [176],\n",
       "       [177],\n",
       "       [178],\n",
       "       [179],\n",
       "       [180],\n",
       "       [181],\n",
       "       [182],\n",
       "       [183],\n",
       "       [184],\n",
       "       [185],\n",
       "       [186],\n",
       "       [187],\n",
       "       [188],\n",
       "       [189],\n",
       "       [190],\n",
       "       [191],\n",
       "       [192],\n",
       "       [193],\n",
       "       [194],\n",
       "       [195],\n",
       "       [196],\n",
       "       [197],\n",
       "       [198],\n",
       "       [199],\n",
       "       [200],\n",
       "       [201],\n",
       "       [202],\n",
       "       [203],\n",
       "       [204],\n",
       "       [205],\n",
       "       [206],\n",
       "       [207],\n",
       "       [208],\n",
       "       [209],\n",
       "       [210],\n",
       "       [211],\n",
       "       [212],\n",
       "       [213],\n",
       "       [214],\n",
       "       [215],\n",
       "       [216],\n",
       "       [217],\n",
       "       [218],\n",
       "       [219],\n",
       "       [220],\n",
       "       [221],\n",
       "       [222],\n",
       "       [223],\n",
       "       [224],\n",
       "       [225],\n",
       "       [226],\n",
       "       [227],\n",
       "       [228],\n",
       "       [229],\n",
       "       [230],\n",
       "       [231],\n",
       "       [232],\n",
       "       [233],\n",
       "       [234],\n",
       "       [235],\n",
       "       [236],\n",
       "       [237],\n",
       "       [238],\n",
       "       [239],\n",
       "       [240],\n",
       "       [241],\n",
       "       [242],\n",
       "       [243],\n",
       "       [244],\n",
       "       [245],\n",
       "       [246],\n",
       "       [247],\n",
       "       [248],\n",
       "       [249],\n",
       "       [250],\n",
       "       [251],\n",
       "       [252],\n",
       "       [253],\n",
       "       [254],\n",
       "       [255],\n",
       "       [256],\n",
       "       [257],\n",
       "       [258],\n",
       "       [259],\n",
       "       [260],\n",
       "       [261],\n",
       "       [262],\n",
       "       [263],\n",
       "       [264],\n",
       "       [265],\n",
       "       [266],\n",
       "       [267],\n",
       "       [268],\n",
       "       [269],\n",
       "       [270],\n",
       "       [271],\n",
       "       [272],\n",
       "       [273],\n",
       "       [274],\n",
       "       [275],\n",
       "       [276],\n",
       "       [277],\n",
       "       [278],\n",
       "       [279],\n",
       "       [280],\n",
       "       [281],\n",
       "       [282],\n",
       "       [283],\n",
       "       [284],\n",
       "       [285],\n",
       "       [286],\n",
       "       [287],\n",
       "       [288],\n",
       "       [289],\n",
       "       [290],\n",
       "       [291],\n",
       "       [292],\n",
       "       [293],\n",
       "       [294],\n",
       "       [295],\n",
       "       [296],\n",
       "       [297],\n",
       "       [298],\n",
       "       [299],\n",
       "       [300],\n",
       "       [301],\n",
       "       [302],\n",
       "       [303],\n",
       "       [304],\n",
       "       [305],\n",
       "       [306],\n",
       "       [307],\n",
       "       [308],\n",
       "       [309]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr = np.array(list(range(1,310))).reshape(-1,1)\n",
    "gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:57:46.718579Z",
     "start_time": "2020-05-24T13:57:46.706553Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<16627x309 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "i = OneHotEncoder()\n",
    "i.fit(gr)\n",
    "groups_one_hot = i.transform(test_data.group_id.values.reshape(-1,1))\n",
    "groups_one_hot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:57:49.295903Z",
     "start_time": "2020-05-24T13:57:49.291416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16627x309 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:49:47.852596Z",
     "start_time": "2020-05-24T13:49:47.849955Z"
    }
   },
   "outputs": [],
   "source": [
    "#texts_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:57:52.262685Z",
     "start_time": "2020-05-24T13:57:52.258008Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:57:52.751250Z",
     "start_time": "2020-05-24T13:57:52.747875Z"
    }
   },
   "outputs": [],
   "source": [
    "#features = hstack( [texts_vectorized,groups_one_hot] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:21:29.742088Z",
     "start_time": "2020-05-24T13:21:29.734857Z"
    }
   },
   "outputs": [],
   "source": [
    "target = train_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:21:30.189437Z",
     "start_time": "2020-05-24T13:21:30.184603Z"
    }
   },
   "outputs": [],
   "source": [
    "#doc_id = train_data.doc_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:58:01.618180Z",
     "start_time": "2020-05-24T13:58:01.615226Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = test_data.group_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:58:02.080699Z",
     "start_time": "2020-05-24T13:58:02.076293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([130, 130, 130, ..., 309, 309, 309], dtype=int16)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:59:08.842702Z",
     "start_time": "2020-05-24T13:59:08.840292Z"
    }
   },
   "outputs": [],
   "source": [
    "data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:04:17.738140Z",
     "start_time": "2020-05-24T13:59:09.007477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [05:08<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "number = 1\n",
    "for group in tqdm(set(groups)):\n",
    "    number +=1\n",
    "    elements_in_same_group = data.group_id == group\n",
    "#    print(elements_in_same_group)\n",
    "    vectorizer.fit(data.title[elements_in_same_group] +data.text[elements_in_same_group])\n",
    "    texts_vectorized_titles = vectorizer.transform(data.title[elements_in_same_group])\n",
    "    texts_vectorized_titles = scaler.fit_transform(texts_vectorized_titles)\n",
    "    \n",
    "    texts_vectorized_texts = vectorizer.transform(data.text[elements_in_same_group])\n",
    "    texts_vectorized_texts = scaler.fit_transform(texts_vectorized_texts)\n",
    "\n",
    "    #dist_matr = pairwise_distances(texts_vectorized.tocsr()[elements_in_same_group], texts_vectorized.tocsr()[elements_in_same_group], metric = 'cosine')\n",
    "    dist_matr = pairwise_distances(texts_vectorized_titles, texts_vectorized_titles, metric = 'cosine')\n",
    "    dist_matr2 = pairwise_distances(texts_vectorized_texts, texts_vectorized_texts, metric = 'cosine')\n",
    "    dist_matr3 = pairwise_distances(texts_vectorized_texts, texts_vectorized_titles, metric = 'cosine')\n",
    "    #print(texts_vectorized.shape)\n",
    "    #clustering = DBSCAN(eps = 0.5, min_samples=3).fit(dist_matr)\n",
    "    dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:21][:,::-1]\n",
    "    dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:21][:,::-1]\n",
    "    dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:21][:,::-1]\n",
    "    mean_dist = dist_matr.mean(axis = 1)\n",
    "    dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist.reshape(-1,1)))\n",
    "    dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted2))\n",
    "    dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted3))\n",
    "\n",
    "#    dist_matr_sorted = hstack((dist_matr_sorted, texts_vectorized))\n",
    "    \n",
    "    if number == 2 :\n",
    "        features_by_dist =  dist_matr_sorted\n",
    "    else:\n",
    "        features_by_dist = vstack((features_by_dist, dist_matr_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:04:39.495771Z",
     "start_time": "2020-05-24T14:04:39.478846Z"
    }
   },
   "outputs": [],
   "source": [
    "features_by_dist = hstack([features_by_dist, groups_one_hot])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:04:40.236959Z",
     "start_time": "2020-05-24T14:04:39.929563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 373)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_by_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:56:29.469329Z",
     "start_time": "2020-05-24T13:56:24.322398Z"
    }
   },
   "outputs": [],
   "source": [
    "#SAVE model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "model = RandomForestClassifier(n_jobs=6,n_estimators=200)\n",
    "model.fit(features_by_dist.tocsr(), target)\n",
    "save_obj(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:56:38.304216Z",
     "start_time": "2020-05-24T13:56:29.475150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "[0.7113783533765031]\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "[0.7113783533765031, 0.7077534791252486]\n",
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "[0.7113783533765031, 0.7077534791252486, 0.7351885098743268]\n",
      "0.7181067807920262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "max_score = 0\n",
    "scores = []\n",
    "for train, test in get_splits(train_data,n=3):\n",
    "            print(type(features_by_dist))\n",
    "            model = RandomForestClassifier(n_jobs=6,n_estimators=200)\n",
    "            model.fit(features_by_dist.tocsr()[train], target[train])\n",
    "            pred = (model.predict(features_by_dist.tocsr()[test]))               \n",
    "            scores.append(f1_score(pred,target[test]))\n",
    "            print(scores)\n",
    "print(sum(scores)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:04:46.400215Z",
     "start_time": "2020-05-24T14:04:46.045967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n"
     ]
    }
   ],
   "source": [
    "#SUBMIT\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "print(type(features_by_dist))\n",
    "model = load_obj('model') \n",
    "pred = (model.predict(features_by_dist.tocsr()))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:04:54.432530Z",
     "start_time": "2020-05-24T14:04:54.428666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:05:15.364714Z",
     "start_time": "2020-05-24T14:05:15.360987Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['target'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:08:13.674845Z",
     "start_time": "2020-05-24T14:08:13.624151Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.to_csv('sub',index=False,columns=['pair_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
