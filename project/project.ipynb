{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Итоговый проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:44:47.465331Z",
     "start_time": "2020-05-28T17:44:25.751827Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir\n",
    "import codecs\n",
    "import pickle                                                                   \n",
    "from bs4 import BeautifulSoup as BS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:44:47.471043Z",
     "start_time": "2020-05-28T17:44:47.467891Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:44:47.615116Z",
     "start_time": "2020-05-28T17:44:47.474747Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):                                                       \n",
    "    with open(name + '.pkl', 'wb') as f:                                        \n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "                                                                                \n",
    "def load_obj(name ):                                                            \n",
    "    with open(name + '.pkl', 'rb') as f:                                        \n",
    "        return pickle.load(f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:38:52.520499Z",
     "start_time": "2020-05-28T18:38:51.861136Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    words = text.split() # разбиваем текст на слова\n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "\n",
    "    return ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:41:24.983771Z",
     "start_time": "2020-05-28T12:41:01.221113Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'NumWordsRulesExtractor'\n",
    "#name = 'ArticleExtractor'\n",
    "name = 'train_data'+name\n",
    "train_data = load_obj(name)#Загружаем готовый распарсенный текст уже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:28:52.743042Z",
     "start_time": "2020-05-28T17:44:47.625008Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: NumWordsRulesExtractor\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c29ce26b6a4824a537748a46ff4dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28026), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 6770.dat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#это парсинг теста и доп фич (meta)\n",
    "if submit:\n",
    "    test_data= pd.read_csv('test_groups.csv',dtype=np.int16)\n",
    "\n",
    "    parser_type = 'NumWordsRulesExtractor'\n",
    "    def save_obj(obj, name ):                                                       \n",
    "        with open(name + '.pkl', 'wb') as f:                                        \n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "\n",
    "    def load_obj(name ):                                                            \n",
    "        with open(name + '.pkl', 'rb') as f:                                        \n",
    "            return pickle.load(f)                                                   \n",
    "        \n",
    "    description = {}                                                                      \n",
    "    keywords= {}                                                                      \n",
    "    test_data['description'] = ''\n",
    "    test_data['keywords'] = ''\n",
    "\n",
    "    path = 'content/'                                                               \n",
    "    texts = {}                                                                      \n",
    "    print('Start: ' + parser_type)                                                  \n",
    "    test_data = pd.read_csv('test_groups.csv',dtype=np.int16)                      \n",
    "    #for filename in ['6770.dat']:                                                  \n",
    "    for filename in tqdm(listdir(path)):                                            \n",
    "        doc_id = int(filename.strip('.dat'))                                        \n",
    "        if doc_id not in test_data.doc_id.values:                                  \n",
    "            continue                                                                \n",
    "        try:                                                                        \n",
    "            with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "                bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "                meta = bs.find(\"meta\", {\"name\":\"description\"})\n",
    "                if meta:\n",
    "                    description[doc_id] = meta.get('content','')\n",
    "                meta = bs.find(\"meta\", {\"name\":\"keywords\"})\n",
    "                if meta:\n",
    "                    keywords[doc_id] = meta.get('content','')\n",
    "                extractor = Extractor(extractor=parser_type, html=html)             \n",
    "                s = extractor.getText()                                             \n",
    "                s=s.replace('\\n',\" \")                                               \n",
    "                s=s.replace('\\t',\" \")                                               \n",
    "                s=s.replace('\\r',\" \")                                               \n",
    "                texts[doc_id] = s                                                   \n",
    "        except UnicodeDecodeError: \n",
    "            print('error {}'.format(filename))                                      \n",
    "            with codecs.open(path + filename, 'r', 'utf-8') as f:                   \n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "            bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "            for s in bs.select('script'):                                           \n",
    "                s.extract()                                                         \n",
    "            s = bs.get_text()                                                       \n",
    "            s=s.replace('\\n',\"\")                                                    \n",
    "            s=s.replace('\\t',\"\")                                                    \n",
    "            s=s.replace('\\r',\"\")                                                    \n",
    "            texts[doc_id] = s                                                       \n",
    "    test_data['text'] = test_data.apply(lambda row: texts[row.doc_id], axis = 1)  \n",
    "    test_data['description'] = test_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "    test_data['keywords'] = test_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  \n",
    "else:\n",
    "    path = 'content/'                                                               \n",
    "    description = {}                                                                      \n",
    "    keywords= {}                                                                      \n",
    "    train_data['description'] = ''\n",
    "    train_data['keywords'] = ''\n",
    "    #for filename in ['1.dat']:                                                  \n",
    "    for filename in tqdm(listdir(path)):                                            \n",
    "        doc_id = int(filename.strip('.dat'))                                        \n",
    "        if doc_id not in train_data.doc_id.values:                                  \n",
    "            continue                                                                \n",
    "        with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            url = f.readline().strip()                                          \n",
    "    #        urls[doc_id] = url                                                       \n",
    "            html = f.read()                                                     \n",
    "            bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "            meta = bs.find(\"meta\", {\"name\":\"description\"})\n",
    "            if meta:\n",
    "                description[doc_id] = lemmatize(meta.get('content',''))\n",
    "            meta = bs.find(\"meta\", {\"name\":\"keywords\"})\n",
    "            if meta:\n",
    "                keywords[doc_id] = lemmatize(meta.get('content',''))\n",
    "    train_data['description'] = train_data.apply(lambda row: description.get(row.doc_id,''), axis = 1)  \n",
    "    train_data['keywords'] = train_data.apply(lambda row: keywords.get(row.doc_id,''), axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:38:05.912667Z",
     "start_time": "2020-05-28T18:38:05.907877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11988, 8963)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description),len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T14:38:17.263791Z",
     "start_time": "2020-05-28T14:36:08.341858Z"
    }
   },
   "outputs": [],
   "source": [
    "save_obj(train_data, name + 'lem_meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T12:01:25.426598Z",
     "start_time": "2020-05-27T12:01:25.120004Z"
    }
   },
   "outputs": [],
   "source": [
    "str = train_data.urls[0]\n",
    "import re\n",
    "train_data['urls'] = train_data.apply(lambda row: re.split(';|,|\\.|\\_|\\-|\\*|\\/|\\&|\\?',row.urls), axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:38:14.873984Z",
     "start_time": "2020-05-28T18:38:14.750690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16627\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding = 'UTF-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "if submit:\n",
    "    test_doc_titles = []\n",
    "    for id in test_data.doc_id.values:\n",
    "        test_doc_titles.append(doc_to_title[id])\n",
    "    test_data['title'] = test_doc_titles\n",
    "    print(len(test_doc_titles))\n",
    "else:\n",
    "    doc_titles = []\n",
    "    for id in train_data.doc_id.values:\n",
    "        doc_titles.append(doc_to_title[id])\n",
    "    train_data['title'] = doc_titles\n",
    "    print(len(doc_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:43:52.732572Z",
     "start_time": "2020-05-28T17:43:52.604904Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-45109238d9ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T14:48:14.842760Z",
     "start_time": "2020-05-28T14:48:14.822577Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:39:23.484111Z",
     "start_time": "2020-05-28T18:38:55.058121Z"
    }
   },
   "outputs": [],
   "source": [
    "if submit:\n",
    "    test_data['lem_title'] = test_data.apply(lambda row:lemmatize(row.title) , axis = 1)  \n",
    "else:\n",
    "    train_data['lem_title'] = train_data.apply(lambda row:lemmatize(row.title) , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:39:35.498086Z",
     "start_time": "2020-05-28T18:39:34.835379Z"
    }
   },
   "outputs": [],
   "source": [
    "if submit:\n",
    "    test_data['sum_title'] = test_data.apply(lambda row: row.lem_title +row.description + row.keywords , axis = 1)  \n",
    "else:\n",
    "    train_data['sum_title'] = train_data.apply(lambda row: row.lem_title +row.description + row.keywords , axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:43:07.502870Z",
     "start_time": "2020-05-28T12:43:06.942953Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data['urls'] = train_data.apply(lambda row:lemmatize(row.urls) , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:43:07.575596Z",
     "start_time": "2020-05-28T12:43:07.532545Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# # Now you can use `progress_apply` instead of `apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:43:08.065088Z",
     "start_time": "2020-05-28T12:43:08.062133Z"
    }
   },
   "outputs": [],
   "source": [
    "#train_data['text'] = train_data.progress_apply(lambda row:lemmatize(row.text[0:1000]) , axis = 1)  \n",
    "# train_data['text'] = train_data.progress_apply(lambda row: row.text[0:len(row.title)] , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T12:43:08.545294Z",
     "start_time": "2020-05-28T12:43:08.543080Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "# train_data['text'] = train_data.progress_apply(lambda row: re.sub(r'\\d+', '', row.text) , axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T14:48:43.568653Z",
     "start_time": "2020-05-28T14:48:43.562826Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_splits(data, n=3, random_state=None):\n",
    "    groups = train_data.group_id.unique()\n",
    "    kf = KFold(n_splits=n, shuffle=True, random_state=random_state)\n",
    "    for i,j in kf.split(groups):\n",
    "        train, test = data.group_id.isin(groups[i].reshape(-1)), data.group_id.isin(groups[j].reshape(-1))\n",
    "        yield (train[train].index.values, test[test].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:39:45.811055Z",
     "start_time": "2020-05-28T18:39:45.805403Z"
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(ngram_range = (1,3))#max_features = 70)\n",
    "stop_words =['без','в','вне','во','для','до','за','И','из','изо','к','ко','меж','на','над','о','об','обо','от','ото','по','под','при','про','с', 'а','же','и','ибо','или','как','но','раз','чем','что','не','ни','ли','ль','вот','вон','это','то','ну','как','же','ни','уж','ну','я', 'ты', 'он', 'мы', 'вы', 'её', 'их','кто','тот']\n",
    "\n",
    " \n",
    "vectorizer = CountVectorizer()#max_features = 70)\n",
    "#vectorizer = TfidfVectorizer(stop_words=stop_words)#max_features = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:39:47.279053Z",
     "start_time": "2020-05-28T18:39:47.228942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "all_groups = np.array(list(range(1,310))).reshape(-1,1)\n",
    "i = OneHotEncoder()\n",
    "i.fit(all_groups)\n",
    "if submit:\n",
    "    groups_one_hot_test   = i.transform(test_data.group_id.values.reshape(-1,1))\n",
    "else:\n",
    "    groups_one_hot_train = i.transform(train_data.group_id.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:39:50.546313Z",
     "start_time": "2020-05-28T18:39:50.537908Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "\n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(stop_words=stop_words).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [i[0] for i in words_freq[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:49:17.500724Z",
     "start_time": "2020-05-28T18:49:17.457542Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    number = 1\n",
    "    data['first_pop'] =0\n",
    "    for group in tqdm(data.group_id.unique()):\n",
    "        number +=1\n",
    "        elements_in_same_group = data.group_id == group\n",
    "        #words = data.title[elements_in_same_group] + data.text[elements_in_same_group]\n",
    "        words = data.lem_title[elements_in_same_group]\n",
    "        vectorizer.fit(words)\n",
    "        texts_vectorized_titles = vectorizer.transform(data.lem_title[elements_in_same_group])\n",
    "        texts_vectorized_titles = scaler.fit_transform(texts_vectorized_titles)\n",
    "\n",
    "        texts_vectorized_texts = vectorizer.transform(data.text[elements_in_same_group])\n",
    "        texts_vectorized_texts = scaler.fit_transform(texts_vectorized_texts)\n",
    "        texts_vectorized_keywords = vectorizer.transform(data.keywords[elements_in_same_group])\n",
    "        texts_vectorized_keywords = scaler.fit_transform(texts_vectorized_keywords)\n",
    "        texts_vectorized_description= vectorizer.transform(data.description[elements_in_same_group])\n",
    "        texts_vectorized_description= scaler.fit_transform(texts_vectorized_description)\n",
    "    \n",
    "        texts_vectorized_sumtitles = vectorizer.transform(data.sum_title[elements_in_same_group])\n",
    "        texts_vectorized_sumtitles = scaler.fit_transform(texts_vectorized_sumtitles)\n",
    "        \n",
    "        dist_matr = pairwise_distances(texts_vectorized_titles, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr2 = pairwise_distances(texts_vectorized_texts, texts_vectorized_texts, metric = 'cosine')\n",
    "        dist_matr3 = pairwise_distances(texts_vectorized_texts, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr4 = pairwise_distances(texts_vectorized_keywords, texts_vectorized_keywords, metric = 'cosine')\n",
    "        dist_matr5 = pairwise_distances(texts_vectorized_description, texts_vectorized_description, metric = 'cosine')\n",
    "        dist_matr6 = pairwise_distances(texts_vectorized_sumtitles, texts_vectorized_texts, metric = 'cosine')\n",
    "        dist_matr7 = pairwise_distances(texts_vectorized_keywords, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr8 = pairwise_distances(texts_vectorized_description, texts_vectorized_titles, metric = 'cosine')\n",
    "\n",
    "        #clustering = DBSCAN(eps = 1.3, min_samples=5).fit(dist_matr)\n",
    "        #clf = IsolationForest(random_state=0).fit_predict(dist_matr)\n",
    "\n",
    "        dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:21]\n",
    "        dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:21]\n",
    "        dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:21]\n",
    "        dist_matr_sorted4 = np.sort(dist_matr4,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted5 = np.sort(dist_matr5,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted6 = np.sort(dist_matr6,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted7 = np.sort(dist_matr7,axis=1)[:,:-1][:,0:21]\n",
    "        dist_matr_sorted8 = np.sort(dist_matr7,axis=1)[:,:-1][:,0:21]\n",
    "#         dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:min(50,dist_matr.shape[1])]\n",
    "#         dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:min(50,dist_matr2.shape[1])]\n",
    "#         dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:min(50,dist_matr3.shape[1])]\n",
    "\n",
    "#         if dist_matr_sorted.shape[1] < 50:\n",
    "#             dist_matr_sorted = np.hstack((dist_matr_sorted, np.tile(dist_matr_sorted[:,-1:], (1, 50-dist_matr_sorted.shape[1]))))\n",
    "#             dist_matr_sorted2 = np.hstack((dist_matr_sorted2, np.tile(dist_matr_sorted2[:,-1:], (1, 50-dist_matr_sorted2.shape[1]))))\n",
    "#             dist_matr_sorted3 = np.hstack((dist_matr_sorted3, np.tile(dist_matr_sorted3[:,-1:], (1, 50-dist_matr_sorted3.shape[1]))))\n",
    "\n",
    "\n",
    "        mean_dist = dist_matr.mean(axis = 1)\n",
    "        mean_dist2 = dist_matr2.mean(axis = 1)\n",
    "        mean_dist3 = dist_matr3.mean(axis = 1)\n",
    "        mean_dist4 = dist_matr4.mean(axis = 1)\n",
    "        mean_dist5 = dist_matr5.mean(axis = 1)\n",
    "        mean_dist6 = dist_matr6.mean(axis = 1)\n",
    "        mean_dist7 = dist_matr7.mean(axis = 1)\n",
    "        mean_dist8 = dist_matr8.mean(axis = 1)\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist2.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist3.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist4.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist5.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist6.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist7.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist8.reshape(-1,1)))\n",
    "#         mean_dist = mean_dist *mean_dist \n",
    "#         mean_dist2 = mean_dist2 *mean_dist2 \n",
    "#         mean_dist3 = mean_dist3 *mean_dist3 \n",
    "#         mean_dist4 = mean_dist4 *mean_dist4 \n",
    "#         mean_dist5 = mean_dist5 *mean_dist5 \n",
    "#         mean_dist6 = mean_dist6 *mean_dist6 \n",
    "#         mean_dist7 = mean_dist7 *mean_dist7 \n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist2.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist3.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist4.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist5.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist6.reshape(-1,1)))\n",
    "#         dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist7.reshape(-1,1)))\n",
    "        \n",
    "#         pop_words = get_top_n_words(words,n=100)\n",
    "#         for pop_word in pop_words:\n",
    "#             data.loc[elements_in_same_group,'first_pop']= data[elements_in_same_group].apply(lambda row: int(pop_word in row.text) , axis = 1)  \n",
    "#             pop  = csr_matrix(data[elements_in_same_group].first_pop.values)\n",
    "#             print(pop_word,pop.toarray())\n",
    "#             dist_matr_sorted = np.hstack((dist_matr_sorted, pop.toarray().reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted2))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted3))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted4))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted5))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted6))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted7))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted8))\n",
    "#        dist_matr_sorted = np.hstack((dist_matr_sorted, clustering.labels_.reshape(-1,1)))\n",
    "#        dist_matr_sorted = hstack((dist_matr_sorted, texts_vectorized))\n",
    "\n",
    "        if number == 2 :\n",
    "            features_by_dist =  dist_matr_sorted\n",
    "        else:\n",
    "            features_by_dist = vstack((features_by_dist, dist_matr_sorted))\n",
    "    return features_by_dist.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:51:03.717863Z",
     "start_time": "2020-05-28T18:49:19.180451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19453cb4ed3549ae829b7392b01dfe7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=180), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if submit:\n",
    "    features_by_dist = get_features(test_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_test])\n",
    "    features_by_dist = features_by_dist.tocsr()\n",
    "else:\n",
    "    features_by_dist = get_features(train_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_train])\n",
    "    features_by_dist = features_by_dist.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:41:26.993417Z",
     "start_time": "2020-05-28T18:41:26.985152Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-66b8da27fee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "target = train_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:43:03.595178Z",
     "start_time": "2020-05-28T17:35:28.888890Z"
    }
   },
   "outputs": [],
   "source": [
    "#VALIDATION\n",
    "max_score = 0\n",
    "scores = []\n",
    "pca = PCA(n_components=4)\n",
    "#j = pca.fit_transform(features_by_dist.toarray())\n",
    "j = features_by_dist\n",
    "for i in tqdm(range(5)):\n",
    "    for train, test in get_splits(train_data,n=3,random_state=i):\n",
    "        model = RandomForestClassifier(n_jobs=6,n_estimators=1000)\n",
    "#        model = GradientBoostingClassifier(n_estimators=2000, learning_rate=0.002, max_depth=9)\n",
    "#        model = LogisticRegression(n_jobs=6)pca = PCA(n_components=2)\n",
    "#        model.fit(features_by_dist[train], target[train])\n",
    "        model.fit(j[train], target[train])\n",
    "        pred = model.predict(j[test])               \n",
    "        score = f1_score(pred,train_data.target[test])\n",
    "        scores.append(score)\n",
    "        print(score, end=' ')\n",
    "    print()\n",
    "\n",
    "print('\\nAvg score:' ,sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T17:35:28.878038Z",
     "start_time": "2020-05-28T17:34:26.500326Z"
    }
   },
   "outputs": [],
   "source": [
    "#SAVE model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "model = RandomForestClassifier(n_jobs=6,n_estimators=1000)\n",
    "model.fit(features_by_dist, target)\n",
    "save_obj(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:51:26.340129Z",
     "start_time": "2020-05-28T18:51:23.769996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16627,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SUBMIT\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "print(type(features_by_dist))\n",
    "model = load_obj('model') \n",
    "pred = model.predict(features_by_dist)               \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T18:51:30.077344Z",
     "start_time": "2020-05-28T18:51:29.890382Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['target'] = pred\n",
    "test_data.to_csv('sub_with_meta',index=False,columns=['pair_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T20:08:37.104082Z",
     "start_time": "2020-05-27T20:08:35.524395Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "gr = int(input())\n",
    "print(gr)\n",
    "pca = PCA(n_components=components=2)\n",
    "i = pca.fit_transform(features_by_dist[train_data[train_data.group_id==gr].index.values].toarray())\n",
    "a = plt.subplot()\n",
    "a.scatter(i[:, 0], i[:, 1], c=target[train_data[train_data.group_id==gr].index.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-28T19:33:16.344Z"
    }
   },
   "outputs": [],
   "source": [
    "save_obj(test_data,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
