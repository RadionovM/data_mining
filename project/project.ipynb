{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Итоговый проект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:09:41.253791Z",
     "start_time": "2020-05-27T09:09:22.927418Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from os import listdir\n",
    "import codecs\n",
    "import pickle                                                                   \n",
    "from bs4 import BeautifulSoup as BS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from boilerpipe.extract import Extractor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:09:41.272381Z",
     "start_time": "2020-05-27T09:09:41.261931Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:09:41.451427Z",
     "start_time": "2020-05-27T09:09:41.278606Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv',dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:09:41.526000Z",
     "start_time": "2020-05-27T09:09:41.455916Z"
    }
   },
   "outputs": [],
   "source": [
    "#это парсинг теста\n",
    "if submit:\n",
    "    test_data= pd.read_csv('test_groups.csv',dtype=np.int16)\n",
    "\n",
    "    parser_type = 'NumWordsRulesExtractor'\n",
    "    def save_obj(obj, name ):                                                       \n",
    "        with open(name + '.pkl', 'wb') as f:                                        \n",
    "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "\n",
    "    def load_obj(name ):                                                            \n",
    "        with open(name + '.pkl', 'rb') as f:                                        \n",
    "            return pickle.load(f)                                                   \n",
    "\n",
    "    path = 'content/'                                                               \n",
    "    texts = {}                                                                      \n",
    "    print('Start: ' + parser_type)                                                  \n",
    "    test_data = pd.read_csv('test_groups.csv',dtype=np.int16)                      \n",
    "    #for filename in ['6770.dat']:                                                  \n",
    "    for filename in tqdm(listdir(path)):                                            \n",
    "        doc_id = int(filename.strip('.dat'))                                        \n",
    "        if doc_id not in test_data.doc_id.values:                                  \n",
    "            continue                                                                \n",
    "        try:                                                                        \n",
    "            with codecs.open(path + filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "                extractor = Extractor(extractor=parser_type, html=html)             \n",
    "                s = extractor.getText()                                             \n",
    "                s=s.replace('\\n',\" \")                                               \n",
    "                s=s.replace('\\t',\" \")                                               \n",
    "                s=s.replace('\\r',\" \")                                               \n",
    "                texts[doc_id] = s                                                   \n",
    "        except UnicodeDecodeError: \n",
    "            print('error {}'.format(filename))                                      \n",
    "            with codecs.open(path + filename, 'r', 'utf-8') as f:                   \n",
    "                url = f.readline().strip()                                          \n",
    "                html = f.read()                                                     \n",
    "            bs = BS(html, 'html.parser')#ищем след страницу                         \n",
    "            for s in bs.select('script'):                                           \n",
    "                s.extract()                                                         \n",
    "            s = bs.get_text()                                                       \n",
    "            s=s.replace('\\n',\"\")                                                    \n",
    "            s=s.replace('\\t',\"\")                                                    \n",
    "            s=s.replace('\\r',\"\")                                                    \n",
    "            texts[doc_id] = s                                                       \n",
    "    test_data['text'] = test_data.apply(lambda row: texts[row.doc_id], axis = 1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:09:41.671151Z",
     "start_time": "2020-05-27T09:09:41.530131Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):                                                       \n",
    "    with open(name + '.pkl', 'wb') as f:                                        \n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)                            \n",
    "                                                                                \n",
    "def load_obj(name ):                                                            \n",
    "    with open(name + '.pkl', 'rb') as f:                                        \n",
    "        return pickle.load(f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:10:04.644475Z",
     "start_time": "2020-05-27T09:09:41.694401Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'NumWordsRulesExtractor'\n",
    "name = 'train_data'+name\n",
    "train_data = load_obj(name)#Загружаем готовый распарсенный текст уже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:10:05.064190Z",
     "start_time": "2020-05-27T09:10:04.649179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11690\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding = 'UTF-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "doc_titles = []\n",
    "for id in train_data.doc_id.values:\n",
    "    doc_titles.append(doc_to_title[id])\n",
    "train_data['title'] = doc_titles\n",
    "if submit:\n",
    "    test_doc_titles = []\n",
    "    for id in test_data.doc_id.values:\n",
    "        test_doc_titles.append(doc_to_title[id])\n",
    "    test_data['title'] = test_doc_titles\n",
    "print(len(doc_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:21:32.625372Z",
     "start_time": "2020-05-27T09:21:32.616142Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_splits(data, n=3, random_state=None):\n",
    "    groups = train_data.group_id.unique()\n",
    "    kf = KFold(n_splits=n, shuffle=True, random_state=random_state)\n",
    "    for i,j in kf.split(groups):\n",
    "        train, test = data.group_id.isin(groups[i].reshape(-1)), data.group_id.isin(groups[j].reshape(-1))\n",
    "        yield (train[train].index.values, test[test].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:10:05.286068Z",
     "start_time": "2020-05-27T09:10:05.204562Z"
    }
   },
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(ngram_range = (1,3))#max_features = 70)\n",
    "vectorizer = CountVectorizer()#max_features = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:10:25.840660Z",
     "start_time": "2020-05-27T09:10:25.829699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "all_groups = np.array(list(range(1,310))).reshape(-1,1)\n",
    "i = OneHotEncoder()\n",
    "i.fit(all_groups)\n",
    "groups_one_hot_train = i.transform(train_data.group_id.values.reshape(-1,1))\n",
    "if submit:\n",
    "    groups_one_hot_test   = i.transform(test_data.group_id.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:11:39.645336Z",
     "start_time": "2020-05-27T09:11:39.630630Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    number = 1\n",
    "    for group in tqdm(data.group_id.unique()):\n",
    "        number +=1\n",
    "        elements_in_same_group = data.group_id == group\n",
    "\n",
    "        vectorizer.fit(data.title[elements_in_same_group] + data.text[elements_in_same_group])\n",
    "        texts_vectorized_titles = vectorizer.transform(data.title[elements_in_same_group])\n",
    "        texts_vectorized_titles = scaler.fit_transform(texts_vectorized_titles)\n",
    "\n",
    "        texts_vectorized_texts = vectorizer.transform(data.text[elements_in_same_group])\n",
    "        texts_vectorized_texts = scaler.fit_transform(texts_vectorized_texts)\n",
    "\n",
    "        dist_matr = pairwise_distances(texts_vectorized_titles, texts_vectorized_titles, metric = 'cosine')\n",
    "        dist_matr2 = pairwise_distances(texts_vectorized_texts, texts_vectorized_texts, metric = 'cosine')\n",
    "        dist_matr3 = pairwise_distances(texts_vectorized_texts, texts_vectorized_titles, metric = 'cosine')\n",
    "\n",
    "        #clustering = DBSCAN(eps = 1.3, min_samples=5).fit(dist_matr)\n",
    "        #clf = IsolationForest(random_state=0).fit_predict(dist_matr)\n",
    "\n",
    "        dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:21]\n",
    "        dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:21]\n",
    "        dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:21]\n",
    "    #     dist_matr_sorted = np.sort(dist_matr,axis=1)[:,0:min(50,dist_matr.shape[1])]\n",
    "    #     dist_matr_sorted2 = np.sort(dist_matr2,axis=1)[:,0:min(50,dist_matr2.shape[1])]\n",
    "    #     dist_matr_sorted3 = np.sort(dist_matr3,axis=1)[:,0:min(50,dist_matr3.shape[1])]\n",
    "\n",
    "    #     if dist_matr_sorted.shape[1] < 50:\n",
    "    #         dist_matr_sorted = np.hstack((dist_matr_sorted, np.tile(dist_matr_sorted[:,-1:], (1, 50-dist_matr_sorted.shape[1]))))\n",
    "    #         dist_matr_sorted2 = np.hstack((dist_matr_sorted2, np.tile(dist_matr_sorted2[:,-1:], (1, 50-dist_matr_sorted2.shape[1]))))\n",
    "    #         dist_matr_sorted3 = np.hstack((dist_matr_sorted3, np.tile(dist_matr_sorted3[:,-1:], (1, 50-dist_matr_sorted3.shape[1]))))\n",
    "\n",
    "\n",
    "        mean_dist = dist_matr.mean(axis = 1)\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, mean_dist.reshape(-1,1)))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted2))\n",
    "        dist_matr_sorted = np.hstack((dist_matr_sorted, dist_matr_sorted3))\n",
    "       # dist_matr_sorted = np.hstack((dist_matr_sorted, clustering.labels_.reshape(-1,1)))\n",
    "    #   dist_matr_sorted = hstack((dist_matr_sorted, texts_vectorized))\n",
    "\n",
    "        if number == 2 :\n",
    "            features_by_dist =  dist_matr_sorted\n",
    "        else:\n",
    "            features_by_dist = vstack((features_by_dist, dist_matr_sorted))\n",
    "    return features_by_dist.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:18:48.302032Z",
     "start_time": "2020-05-27T09:11:39.812561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92192a6b9fb040f0b770228df939751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if submit:\n",
    "    features_by_dist = get_features(test_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_test])\n",
    "    features_by_dist = features_by_dist.tocsr()\n",
    "else:\n",
    "    target = train_data.target.values\n",
    "    features_by_dist = get_features(train_data)\n",
    "    features_by_dist = hstack([features_by_dist, groups_one_hot_train])\n",
    "    features_by_dist = features_by_dist.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T09:27:06.832051Z",
     "start_time": "2020-05-27T09:25:30.264538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d3c9f6c0df42b48c735998d0c5ddfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723110151187905 0.7288765088207985 0.6989583333333333 \n",
      "0.7031476997578692 0.7317073170731707 0.6998011928429423 \n",
      "0.7544075440754408 0.7258150721539285 0.6407480314960631 \n",
      "0.7545893719806763 0.6940718303764605 0.6893056259503295 \n",
      "0.6771724448446645 0.7218119153126539 0.7543447627994364 \n",
      "\n",
      "\n",
      "Avg score: 0.713191186800378\n"
     ]
    }
   ],
   "source": [
    "#VALIDATION\n",
    "max_score = 0\n",
    "scores = []\n",
    "for i in tqdm(range(5)):\n",
    "    for train, test in get_splits(train_data,n=3,random_state=i):\n",
    "        model = RandomForestClassifier(n_jobs=6,n_estimators=200)\n",
    "        model.fit(features_by_dist[train], target[train])\n",
    "        pred = (model.predict(features_by_dist[test]))               \n",
    "        score = f1_score(pred,target[test])\n",
    "        scores.append(score)\n",
    "        print(score, end=' ')\n",
    "    print()\n",
    "\n",
    "print('\\nAvg score:' ,sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE model\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "model = RandomForestClassifier(n_jobs=6,n_estimators=200)\n",
    "model.fit(features_by_dist, target)\n",
    "save_obj(model, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T21:34:10.267423Z",
     "start_time": "2020-05-26T21:34:08.615849Z"
    }
   },
   "outputs": [],
   "source": [
    "#SUBMIT\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "print(type(features_by_dist))\n",
    "model = load_obj('model') \n",
    "pred = (model.predict(features_by_dist)               \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T21:34:16.126667Z",
     "start_time": "2020-05-26T21:34:16.120798Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data['target'] = pred\n",
    "test_data.to_csv('sub',index=False,columns=['pair_id','target'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "402px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
